# Eval Architecture Options

## ğŸ¯ The Challenge

We need evals at **TWO levels**:
1. **Provider-level** - "How good is this AI response?"
2. **Workflow-level** - "How good is this entire workflow execution?"

And the architecture must support:
- Recording (already designed)
- Replay (already designed)
- **Comparison** - Same input, different configs
- **Scoring** - Quality metrics (automated + human)
- **Regression testing** - Did we break something?
- **Optimization** - Cost vs quality tradeoffs

---

## ğŸ“Š Recording Structure (Foundation - Already Agreed)

We've locked in the recording infrastructure. Now we need to define how evals layer on top:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Workflow Recording                        â”‚
â”‚  id: "wf-123"                                               â”‚
â”‚  workflowId: "coder-reviewer"                               â”‚
â”‚  input: { task: "Build a REST API" }                        â”‚
â”‚  output: { code: "...", tests: "..." }                      â”‚
â”‚                                                              â”‚
â”‚  nodeRecordings: [                                          â”‚
â”‚    { nodeId: "coder", providerRecording: {...} },           â”‚
â”‚    { nodeId: "reviewer", providerRecording: {...} },        â”‚
â”‚    { nodeId: "coder", providerRecording: {...} },  // retry â”‚
â”‚  ]                                                          â”‚
â”‚                                                              â”‚
â”‚  stateSnapshots: [                                          â”‚
â”‚    { after: "coder", state: { code: "v1..." } },            â”‚
â”‚    { after: "reviewer", state: { feedback: "..." } },       â”‚
â”‚    { after: "coder", state: { code: "v2..." } },            â”‚
â”‚  ]                                                          â”‚
â”‚                                                              â”‚
â”‚  metrics: {                                                 â”‚
â”‚    totalDurationMs: 45000,                                  â”‚
â”‚    totalTokens: 15000,                                      â”‚
â”‚    totalCost: 0.15,                                         â”‚
â”‚    nodeCount: 3,                                            â”‚
â”‚    loopIterations: 2,                                       â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Five Eval Architecture Options

### **Option A: Eval as Post-Processing Pipeline**

**Concept:** Evals are completely separate from recording. You record first, then run evals later as a batch process.

```
Recording Phase:          Eval Phase:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow â”‚â”€â”€recordâ”€â”€â”€â–¶ â”‚ Recordingâ”‚â”€â”€â”€â”€â–¶â”‚ Eval     â”‚â”€â”€â”€â”€â–¶ Scores
â”‚   Run    â”‚             â”‚  Store   â”‚     â”‚ Pipeline â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â–¼                 â–¼                 â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Latency â”‚      â”‚ Quality â”‚      â”‚ Cost    â”‚
                         â”‚ Scorer  â”‚      â”‚ Scorer  â”‚      â”‚ Scorer  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API:**

```typescript
// Eval is a separate concern from recording
interface EvalPipeline {
  // Run a single eval
  evaluate(recording: WorkflowRecording, rubric: EvalRubric): Promise<EvalResult>;
  
  // Batch eval
  evaluateMany(recordings: WorkflowRecording[], rubric: EvalRubric): Promise<EvalResult[]>;
  
  // Compare two recordings
  compare(a: WorkflowRecording, b: WorkflowRecording, rubric: EvalRubric): Promise<ComparisonResult>;
}

// Rubric defines what to measure
interface EvalRubric {
  name: string;
  scorers: Scorer[];
  aggregation: "average" | "weighted" | "min" | "custom";
}

// Scorer is a function that produces a score
type Scorer = {
  name: string;
  level: "provider" | "node" | "workflow";
  score: (recording: WorkflowRecording | ProviderRecording) => Promise<Score>;
};

// Built-in scorers
const latencyScorer: Scorer = { name: "latency", level: "workflow", score: (r) => ... };
const costScorer: Scorer = { name: "cost", level: "workflow", score: (r) => ... };
const tokenScorer: Scorer = { name: "tokens", level: "provider", score: (r) => ... };

// LLM-as-judge scorer
function llmJudgeScorer(criteria: string, model?: string): Scorer;

// Human annotation scorer (async, requires human input)
function humanAnnotationScorer(prompt: string): Scorer;
```

**Usage:**

```typescript
// Define rubric
const codeQualityRubric: EvalRubric = {
  name: "Code Quality",
  scorers: [
    latencyScorer,
    costScorer,
    llmJudgeScorer("Rate the code quality from 1-10"),
    llmJudgeScorer("Does the code handle edge cases?"),
  ],
  aggregation: "weighted",
};

// Run evals on batch of recordings
const recordings = await store.list({ workflowId: "coder-reviewer" });
const results = await pipeline.evaluateMany(recordings, codeQualityRubric);

// Compare two configurations
const claudeRuns = await store.list({ workflowId: "coder-reviewer", providerType: "claude" });
const openCodeRuns = await store.list({ workflowId: "coder-reviewer", providerType: "opencode" });
const comparison = await pipeline.compare(claudeRuns[0], openCodeRuns[0], codeQualityRubric);
```

**Pros:**
- âœ… Simple mental model: record â†’ eval (separate steps)
- âœ… Can run evals on historical data
- âœ… Scorers are composable
- âœ… Easy to add new scorers

**Cons:**
- âŒ No real-time scoring
- âŒ LLM-as-judge adds cost/latency
- âŒ Workflow-level scoring may miss context

**Score: 82/100**

---

### **Option B: Eval Hooks in Recording**

**Concept:** Evals are hooks that fire during recording. You can score in real-time as the workflow executes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Recording + Eval                          â”‚
â”‚                                                              â”‚
â”‚  workflow.execute()                                         â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  onNodeComplete  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Node   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Eval    â”‚â”€â”€â–¶ Real-time     â”‚
â”‚  â”‚  Runs   â”‚                  â”‚ Hooks   â”‚    Scores        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  onWorkflowComplete  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Workflowâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Final   â”‚â”€â”€â–¶ Report   â”‚
â”‚  â”‚ Done    â”‚                       â”‚ Eval    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API:**

```typescript
interface EvalHooks {
  // Fire after each provider call
  onProviderComplete?: (recording: ProviderRecording) => Promise<Score | void>;
  
  // Fire after each node completes
  onNodeComplete?: (recording: NodeRecording, state: RunSnapshot) => Promise<Score | void>;
  
  // Fire when workflow completes
  onWorkflowComplete?: (recording: WorkflowRecording) => Promise<EvalResult>;
}

// Recording with hooks
const recordingProvider = withRecording(claudeTrait, {
  mode: "record",
  store,
  evalHooks: {
    onProviderComplete: async (rec) => {
      // Real-time quality check
      return await quickQualityScore(rec);
    },
    onWorkflowComplete: async (rec) => {
      // Full eval at end
      return await fullEval(rec);
    },
  },
});
```

**Pros:**
- âœ… Real-time feedback
- âœ… Can abort early on low scores
- âœ… Scores are attached to recordings
- âœ… Context available during eval

**Cons:**
- âŒ Eval logic coupled to recording
- âŒ Can't easily re-run evals on old data
- âŒ Hooks add latency to workflow

**Score: 78/100**

---

### **Option C: Eval Datasets + Test Suites**

**Concept:** Focus on creating and managing eval datasets. Recordings become test cases. Test suites define expected behavior.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Eval Dataset                              â”‚
â”‚                                                              â”‚
â”‚  name: "coder-reviewer-golden"                              â”‚
â”‚  version: "1.0"                                             â”‚
â”‚                                                              â”‚
â”‚  testCases: [                                               â”‚
â”‚    {                                                        â”‚
â”‚      id: "simple-api",                                      â”‚
â”‚      input: { task: "Build a hello world API" },            â”‚
â”‚      goldenRecording: {...},  // The "correct" answer       â”‚
â”‚      assertions: [                                          â”‚
â”‚        { type: "output_contains", value: "app.get" },       â”‚
â”‚        { type: "latency_under", value: 30000 },             â”‚
â”‚        { type: "cost_under", value: 0.10 },                 â”‚
â”‚      ],                                                     â”‚
â”‚    },                                                       â”‚
â”‚    {                                                        â”‚
â”‚      id: "complex-crud",                                    â”‚
â”‚      input: { task: "Build CRUD API with auth" },           â”‚
â”‚      goldenRecording: {...},                                â”‚
â”‚      assertions: [...],                                     â”‚
â”‚    },                                                       â”‚
â”‚  ]                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API:**

```typescript
// Dataset management
interface EvalDataset {
  id: string;
  name: string;
  version: string;
  workflowId: string;
  testCases: TestCase[];
}

interface TestCase {
  id: string;
  input: unknown;
  goldenRecording?: WorkflowRecording;  // Optional "correct" answer
  assertions: Assertion[];
}

type Assertion =
  | { type: "output_contains"; path: string; value: unknown }
  | { type: "output_equals"; path: string; value: unknown }
  | { type: "latency_under"; value: number }
  | { type: "cost_under"; value: number }
  | { type: "tokens_under"; value: number }
  | { type: "no_errors" }
  | { type: "node_count"; min?: number; max?: number }
  | { type: "llm_judge"; criteria: string; minScore: number }
  | { type: "similarity_to_golden"; minScore: number };

// Test runner
interface EvalRunner {
  // Run a single test case
  runTest(workflow: WorkflowDefinition, testCase: TestCase): Promise<TestResult>;
  
  // Run all tests in dataset
  runDataset(workflow: WorkflowDefinition, dataset: EvalDataset): Promise<DatasetResult>;
  
  // Compare two versions against same dataset
  compareVersions(
    v1: WorkflowDefinition,
    v2: WorkflowDefinition,
    dataset: EvalDataset,
  ): Promise<VersionComparison>;
}

interface TestResult {
  testCaseId: string;
  passed: boolean;
  recording: WorkflowRecording;
  assertionResults: AssertionResult[];
  scores: Record<string, number>;
}

interface DatasetResult {
  datasetId: string;
  passRate: number;
  testResults: TestResult[];
  summary: {
    avgLatency: number;
    avgCost: number;
    avgTokens: number;
  };
}
```

**Usage:**

```typescript
// Define a test suite
const goldenDataset: EvalDataset = {
  id: "coder-reviewer-v1",
  name: "Coder-Reviewer Golden Tests",
  version: "1.0",
  workflowId: "coder-reviewer",
  testCases: [
    {
      id: "hello-world",
      input: { task: "Build a hello world Express API" },
      assertions: [
        { type: "output_contains", path: "code", value: "app.get" },
        { type: "output_contains", path: "code", value: "hello" },
        { type: "latency_under", value: 30000 },
        { type: "no_errors" },
      ],
    },
    {
      id: "with-middleware",
      input: { task: "Build an API with auth middleware" },
      assertions: [
        { type: "output_contains", path: "code", value: "middleware" },
        { type: "llm_judge", criteria: "Does the code properly validate JWT tokens?", minScore: 7 },
      ],
    },
  ],
};

// Run tests
const results = await runner.runDataset(coderReviewerWorkflow, goldenDataset);
console.log(`Pass rate: ${results.passRate * 100}%`);

// Compare versions
const comparison = await runner.compareVersions(workflowV1, workflowV2, goldenDataset);
console.log(`V1 pass rate: ${comparison.v1.passRate}`);
console.log(`V2 pass rate: ${comparison.v2.passRate}`);
```

**Pros:**
- âœ… Clear test/eval mental model
- âœ… Versioned datasets
- âœ… Regression testing built-in
- âœ… Golden tests for determinism
- âœ… Great for CI/CD

**Cons:**
- âŒ Need to create/maintain datasets
- âŒ Less flexible than ad-hoc scoring
- âŒ May not capture all quality dimensions

**Score: 88/100**

---

### **Option D: Multi-Level Eval Engine**

**Concept:** Layered eval system that naturally handles provider, node, and workflow levels. Each layer builds on the previous.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Workflow Eval                             â”‚
â”‚  - Aggregates node evals                                    â”‚
â”‚  - Cross-node consistency                                   â”‚
â”‚  - End-to-end quality                                       â”‚
â”‚  - Total cost/time/tokens                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ composed of
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Node Eval                                 â”‚
â”‚  - State transformation quality                             â”‚
â”‚  - Input/output consistency                                 â”‚
â”‚  - Retries and error handling                               â”‚
â”‚  - Individual node metrics                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ composed of
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Provider Eval                             â”‚
â”‚  - Response quality                                         â”‚
â”‚  - Latency                                                  â”‚
â”‚  - Token usage                                              â”‚
â”‚  - Cost                                                     â”‚
â”‚  - Tool usage patterns                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API:**

```typescript
// Multi-level eval engine
interface EvalEngine {
  // Provider level
  evaluateProvider(recording: ProviderRecording, config: ProviderEvalConfig): Promise<ProviderScore>;
  
  // Node level (includes provider eval + state eval)
  evaluateNode(recording: NodeRecording, config: NodeEvalConfig): Promise<NodeScore>;
  
  // Workflow level (includes all node evals + workflow-specific)
  evaluateWorkflow(recording: WorkflowRecording, config: WorkflowEvalConfig): Promise<WorkflowScore>;
}

// Config at each level
interface ProviderEvalConfig {
  latencyWeight: number;
  costWeight: number;
  qualityCriteria?: string[];  // For LLM-as-judge
}

interface NodeEvalConfig extends ProviderEvalConfig {
  stateValidation?: (before: unknown, after: unknown) => boolean;
  expectedOutputSchema?: ZodSchema;
}

interface WorkflowEvalConfig extends NodeEvalConfig {
  // Cross-node checks
  consistencyChecks?: ConsistencyCheck[];
  // End-to-end quality
  endToEndCriteria?: string[];
  // Expected behavior
  expectedNodeSequence?: string[];
  maxLoopIterations?: number;
}

// Scores at each level
interface ProviderScore {
  overall: number;  // 0-100
  breakdown: {
    latency: number;
    cost: number;
    quality: number;
    toolUsage: number;
  };
  metadata: { tokens: number; durationMs: number; cost: number };
}

interface NodeScore extends ProviderScore {
  stateTransformationScore: number;
  retryCount: number;
  errorHandlingScore: number;
}

interface WorkflowScore {
  overall: number;
  nodeScores: Record<string, NodeScore>;
  aggregateMetrics: {
    totalLatency: number;
    totalCost: number;
    totalTokens: number;
  };
  workflowSpecific: {
    consistencyScore: number;
    completionScore: number;
    efficiencyScore: number;  // Did it take optimal path?
  };
}
```

**Usage:**

```typescript
const engine = createEvalEngine();

// Provider-level eval (just the AI call)
const providerScore = await engine.evaluateProvider(
  recording.nodeRecordings[0].providerRecording,
  { latencyWeight: 0.3, costWeight: 0.3, qualityCriteria: ["code_quality"] }
);

// Node-level eval (AI call + state transformation)
const nodeScore = await engine.evaluateNode(
  recording.nodeRecordings[0],
  {
    latencyWeight: 0.3,
    costWeight: 0.3,
    stateValidation: (before, after) => after.code !== before.code,
  }
);

// Workflow-level eval (entire execution)
const workflowScore = await engine.evaluateWorkflow(recording, {
  latencyWeight: 0.2,
  costWeight: 0.2,
  endToEndCriteria: ["Does the final code work?", "Are all tests passing?"],
  expectedNodeSequence: ["coder", "reviewer", "coder"],  // Expected retry once
  maxLoopIterations: 3,
});

console.log(`Workflow score: ${workflowScore.overall}/100`);
console.log(`Coder node score: ${workflowScore.nodeScores["coder"].overall}/100`);
```

**Pros:**
- âœ… Natural hierarchy (provider â†’ node â†’ workflow)
- âœ… Can eval at any level independently
- âœ… Workflow eval includes context
- âœ… Good for debugging (drill down to problematic node)

**Cons:**
- âŒ More complex API
- âŒ Need to configure at multiple levels
- âŒ May over-engineer for simple use cases

**Score: 85/100**

---

### **Option E: Hybrid Eval System (Recommended â­)**

**Concept:** Combine the best of all approaches:
- **Dataset-driven testing** (from Option C) for CI/CD and regression
- **Multi-level scoring** (from Option D) for analysis
- **Post-processing pipeline** (from Option A) for batch evals
- **Optional hooks** (from Option B) for real-time monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Eval System                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. DATASETS (for testing)                                  â”‚
â”‚     â”œâ”€â”€ Test suites with assertions                         â”‚
â”‚     â”œâ”€â”€ Golden recordings                                   â”‚
â”‚     â””â”€â”€ Regression detection                                â”‚
â”‚                                                              â”‚
â”‚  2. SCORERS (for quality)                                   â”‚
â”‚     â”œâ”€â”€ Built-in: latency, cost, tokens                    â”‚
â”‚     â”œâ”€â”€ LLM-as-judge: quality criteria                     â”‚
â”‚     â””â”€â”€ Custom: user-defined functions                      â”‚
â”‚                                                              â”‚
â”‚  3. COMPARISONS (for analysis)                              â”‚
â”‚     â”œâ”€â”€ Same workflow, different providers                  â”‚
â”‚     â”œâ”€â”€ Same workflow, different versions                   â”‚
â”‚     â””â”€â”€ A/B testing support                                 â”‚
â”‚                                                              â”‚
â”‚  4. HOOKS (for real-time)                                   â”‚
â”‚     â”œâ”€â”€ Optional monitoring                                 â”‚
â”‚     â””â”€â”€ Alert on low scores                                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Complete API:**

```typescript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATASETS (Testing & Regression)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

interface EvalDataset {
  id: string;
  name: string;
  version: string;
  workflowId: string;
  testCases: TestCase[];
}

interface TestCase {
  id: string;
  name: string;
  input: unknown;
  assertions: Assertion[];
  golden?: WorkflowRecording;  // Optional golden recording
  tags?: string[];
}

// Rich assertion types
type Assertion =
  // Output assertions
  | { type: "output.contains"; path: string; value: unknown }
  | { type: "output.equals"; path: string; value: unknown }
  | { type: "output.matches"; path: string; pattern: RegExp }
  | { type: "output.schema"; path: string; schema: ZodSchema }
  
  // Metric assertions
  | { type: "metric.latency"; max: number }
  | { type: "metric.cost"; max: number }
  | { type: "metric.tokens"; max: number }
  
  // Behavior assertions
  | { type: "behavior.no_errors" }
  | { type: "behavior.node_executed"; nodeId: string }
  | { type: "behavior.node_count"; min?: number; max?: number }
  | { type: "behavior.loop_count"; max: number }
  
  // Quality assertions (LLM-as-judge)
  | { type: "quality.llm_judge"; criteria: string; minScore: number }
  | { type: "quality.similarity"; to: "golden"; minScore: number }
  
  // Custom assertion
  | { type: "custom"; fn: (recording: WorkflowRecording) => boolean | Promise<boolean> };

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SCORERS (Quality Measurement)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

interface Scorer {
  name: string;
  level: "provider" | "node" | "workflow";
  score(recording: Recording): Promise<ScorerResult>;
}

interface ScorerResult {
  name: string;
  score: number;  // 0-100
  details?: Record<string, unknown>;
}

// Built-in scorers
const builtInScorers = {
  // Metrics (automatic, no LLM needed)
  latency: createMetricScorer("latency", (r) => r.metrics.durationMs),
  cost: createMetricScorer("cost", (r) => r.metrics.cost),
  tokens: createMetricScorer("tokens", (r) => r.metrics.tokens.total),
  
  // LLM-as-judge (requires LLM call)
  llmJudge: (criteria: string, model?: string) => createLLMScorer(criteria, model),
  
  // Similarity (to golden or another recording)
  similarity: (reference: WorkflowRecording) => createSimilarityScorer(reference),
};

// Custom scorer
function createCustomScorer(
  name: string,
  level: "provider" | "node" | "workflow",
  fn: (recording: Recording) => number | Promise<number>,
): Scorer;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// EVAL ENGINE (Unified Interface)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

interface EvalEngine {
  // === Dataset Operations ===
  
  // Run a single test case
  runTest(
    workflow: WorkflowExecutor,
    testCase: TestCase,
  ): Promise<TestResult>;
  
  // Run entire dataset
  runDataset(
    workflow: WorkflowExecutor,
    dataset: EvalDataset,
  ): Promise<DatasetResult>;
  
  // === Scoring Operations ===
  
  // Score a recording
  score(
    recording: WorkflowRecording,
    scorers: Scorer[],
  ): Promise<ScoredRecording>;
  
  // Score at specific level
  scoreProvider(recording: ProviderRecording, scorers: Scorer[]): Promise<ProviderScore>;
  scoreNode(recording: NodeRecording, scorers: Scorer[]): Promise<NodeScore>;
  scoreWorkflow(recording: WorkflowRecording, scorers: Scorer[]): Promise<WorkflowScore>;
  
  // === Comparison Operations ===
  
  // Compare two recordings
  compare(
    a: WorkflowRecording,
    b: WorkflowRecording,
    scorers?: Scorer[],
  ): Promise<ComparisonResult>;
  
  // Compare across a dimension (e.g., provider type)
  compareAcross(
    recordings: WorkflowRecording[],
    dimension: "provider" | "workflowVersion" | "custom",
    scorers?: Scorer[],
  ): Promise<DimensionComparison>;
  
  // === Reporting ===
  
  report(
    recordings: WorkflowRecording[],
    options: ReportOptions,
  ): Promise<EvalReport>;
}

// Results
interface TestResult {
  testCaseId: string;
  passed: boolean;
  recording: WorkflowRecording;
  assertions: AssertionResult[];
  duration: number;
}

interface DatasetResult {
  datasetId: string;
  version: string;
  passRate: number;
  results: TestResult[];
  summary: {
    passed: number;
    failed: number;
    avgLatency: number;
    avgCost: number;
    avgTokens: number;
  };
}

interface ComparisonResult {
  winner: "a" | "b" | "tie";
  aScores: WorkflowScore;
  bScores: WorkflowScore;
  diff: {
    latency: number;  // a - b (negative = a is faster)
    cost: number;     // a - b (negative = a is cheaper)
    quality: number;  // a - b (positive = a is better)
  };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HOOKS (Real-time Monitoring - Optional)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

interface EvalHooks {
  // Fire after each provider call
  onProviderComplete?: (
    recording: ProviderRecording,
  ) => Promise<void | { alert?: string; score?: number }>;
  
  // Fire after workflow completes
  onWorkflowComplete?: (
    recording: WorkflowRecording,
  ) => Promise<void | EvalResult>;
}

// Attach hooks to recording wrapper
const monitoredProvider = withRecording(claudeTrait, {
  mode: "passthrough",
  store: productionStore,
  evalHooks: {
    onProviderComplete: async (rec) => {
      if (rec.metrics.cost > 0.10) {
        return { alert: "High cost provider call", score: 50 };
      }
    },
  },
});
```

**Usage Examples:**

```typescript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// USE CASE 1: CI/CD Testing
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Define test dataset
const regressionTests: EvalDataset = {
  id: "coder-reviewer-regression",
  name: "Coder-Reviewer Regression Tests",
  version: "1.0",
  workflowId: "coder-reviewer",
  testCases: [
    {
      id: "simple-api",
      name: "Simple Express API",
      input: { task: "Build a hello world Express API" },
      assertions: [
        { type: "output.contains", path: "code", value: "express" },
        { type: "output.contains", path: "code", value: "app.get" },
        { type: "metric.latency", max: 30000 },
        { type: "metric.cost", max: 0.10 },
        { type: "behavior.no_errors" },
      ],
    },
    {
      id: "with-tests",
      name: "API with unit tests",
      input: { task: "Build a REST API with Jest tests" },
      assertions: [
        { type: "output.contains", path: "code", value: "describe(" },
        { type: "quality.llm_judge", criteria: "Are the tests comprehensive?", minScore: 7 },
      ],
    },
  ],
};

// Run in CI
const results = await evalEngine.runDataset(coderReviewerWorkflow, regressionTests);

if (results.passRate < 1.0) {
  console.error("Regression tests failed!");
  process.exit(1);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// USE CASE 2: Provider Comparison (Claude vs OpenCode)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Record runs with both providers
const claudeRecordings = await runWithProvider(workflow, "claude.agent", testInputs);
const openCodeRecordings = await runWithProvider(workflow, "opencode.agent", testInputs);

// Compare
const comparison = await evalEngine.compareAcross(
  [...claudeRecordings, ...openCodeRecordings],
  "provider",
  [builtInScorers.latency, builtInScorers.cost, builtInScorers.llmJudge("code quality")],
);

console.log(`
Provider Comparison:
- Claude: avg latency ${comparison.byDimension["claude.agent"].avgLatency}ms, cost $${comparison.byDimension["claude.agent"].avgCost}
- OpenCode: avg latency ${comparison.byDimension["opencode.agent"].avgLatency}ms, cost $${comparison.byDimension["opencode.agent"].avgCost}
- Quality winner: ${comparison.qualityWinner}
`);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// USE CASE 3: Workflow Version A/B Testing
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const v1Recordings = await store.list({ workflowVersion: "1.0" });
const v2Recordings = await store.list({ workflowVersion: "2.0" });

const abTest = await evalEngine.compare(
  v1Recordings[0],
  v2Recordings[0],
  [
    builtInScorers.latency,
    builtInScorers.cost,
    builtInScorers.llmJudge("overall quality"),
  ],
);

console.log(`A/B Test: ${abTest.winner === "b" ? "V2 wins!" : "V1 wins or tie"}`);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// USE CASE 4: Quality Monitoring in Production
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const productionProvider = withRecording(claudeTrait, {
  mode: "passthrough",
  store: sqliteStore,
  evalHooks: {
    onWorkflowComplete: async (recording) => {
      // Quick quality check
      const score = await evalEngine.score(recording, [
        builtInScorers.cost,
        builtInScorers.llmJudge("user satisfaction"),
      ]);
      
      // Alert if low quality
      if (score.overall < 70) {
        await alertOps("Low quality workflow detected", { recordingId: recording.id });
      }
      
      // Store score for trending
      await metricsDB.insert({
        timestamp: Date.now(),
        recordingId: recording.id,
        score: score.overall,
      });
    },
  },
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// USE CASE 5: Generate Report
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const lastWeekRecordings = await store.list({
  startedAfter: Date.now() - 7 * 24 * 60 * 60 * 1000,
});

const report = await evalEngine.report(lastWeekRecordings, {
  groupBy: ["workflowId", "providerType"],
  metrics: ["latency", "cost", "tokens"],
  includeScores: true,
  scorers: [builtInScorers.llmJudge("output quality")],
});

console.log(report.markdown());  // Generate markdown report
```

**How This Architecture Supports Workflow-Level Evals:**

1. **WorkflowRecording captures everything** - All node recordings, state snapshots, cross-node interactions

2. **Assertions can check workflow behavior** - Node sequences, loop counts, state transformations

3. **Scorers work at any level** - Provider, node, or workflow

4. **Comparisons are workflow-aware** - Compare entire workflow executions, not just individual calls

5. **Golden recordings are full workflows** - Capture expected behavior for entire workflow

**Pros:**
- âœ… Unified system for all eval needs
- âœ… Dataset-driven for CI/CD
- âœ… Multi-level scoring
- âœ… Comparison built-in
- âœ… Optional real-time hooks
- âœ… Supports both provider AND workflow evals
- âœ… Extensible with custom scorers

**Cons:**
- âŒ More complex than simple approaches
- âŒ LLM-as-judge adds cost
- âŒ Need to design good test cases

**Score: 94/100**

---

## ğŸ¯ My Recommendation: Option E (Hybrid Eval System)

**Why:**

1. **Covers all use cases** - Testing, comparison, monitoring, optimization
2. **Works at both levels** - Provider evals AND workflow evals
3. **Progressive complexity** - Start simple (assertions), add LLM-as-judge later
4. **Future-proof** - Can add new scorers, assertions, comparison dimensions
5. **Evidence of workflow support:**
   - WorkflowRecording includes full node sequence
   - Assertions can check cross-node behavior
   - Scorers aggregate across nodes
   - Golden recordings capture expected workflow behavior

---

## ğŸ“‹ Updated Implementation Plan

### Recording Infrastructure (Already Agreed)
- Recording format
- RecordingStore interface
- withRecording wrapper
- Provider trait + adapter

### Eval Infrastructure (New - Option E)
**Phase 1: Core Types (2 hours)**
- Assertion types
- Scorer interface
- TestCase/EvalDataset types

**Phase 2: Built-in Scorers (2 hours)**
- Latency, cost, tokens scorers
- Similarity scorer
- Custom scorer factory

**Phase 3: Dataset Runner (3 hours)**
- runTest / runDataset
- Assertion evaluation
- TestResult / DatasetResult

**Phase 4: LLM-as-Judge Scorer (2 hours)**
- llmJudge scorer
- Configurable model
- Caching for efficiency

**Phase 5: Comparison Engine (2 hours)**
- compare two recordings
- compareAcross dimension
- Winner determination

**Phase 6: Hooks (Optional, 1 hour)**
- onProviderComplete
- onWorkflowComplete

**Total Eval Work: ~12 hours**
**Total Project: ~32-39 hours**

---

Ready to lock this in?