---
title: Replay Testing Model
description: Testing AI workflows with recorded event fixtures
---

# Replay Testing Model

AI workflows are hard to test. LLM responses are non-deterministic. API calls are slow and expensive. Traditional unit testing falls short.

Open Harness solves this with replay testing - record once, replay forever.

## The Problem

### Non-Determinism

The same prompt can produce different outputs:

```typescript
// First run
"Here are 3 key points: 1. Performance 2. Scalability 3. Cost"

// Second run (same prompt)
"Three main considerations: scalability, performance, and cost-effectiveness"
```

How do you assert on output that changes?

### Slow and Expensive

LLM calls take seconds and cost money:

```typescript
// Every test run = real API call
await agent.execute(input);  // 2 seconds, $0.01
```

A test suite with 100 tests would take 200 seconds and cost $1 per run.

### Flaky Tests

Network issues, rate limits, and model updates cause failures:

```typescript
// Today: passes
// Tomorrow: "Rate limit exceeded"
// Next week: "Model deprecated"
```

## The Solution

### Event-Based Testing

Since everything flows through the Hub, capture and replay:

```typescript
// 1. Record during real execution
const events = [];
hub.subscribe("*", (e) => events.push(e));
await harness.run();
saveFixture(events);

// 2. Replay in tests (no API calls)
const fixture = loadFixture();
replayEvents(fixture);
assertBehavior();
```

### Golden Fixtures

Save known-good event sequences:

```
recordings/
  golden/
    happy-path.json
    error-handling.json
    edge-cases.json
```

Tests replay these fixtures instead of making live calls.

## How It Works

### Recording Phase

Run with real APIs, capture events:

```typescript
const recorder = createRecorder();
hub.subscribe("*", recorder.capture);

const result = await harness.run(input);

recorder.save("tests/fixtures/scenario.json");
```

### Replay Phase

Load fixtures, verify behavior:

```typescript
const fixture = loadFixture("tests/fixtures/scenario.json");

// Create mock hub that replays events
const mockHub = createReplayHub(fixture.events);

// Run with mock - no API calls
await executeFlow(flow, {
  hub: mockHub,
  input: fixture.input
});

// Assert on behavior
expect(mockHub.emitted("agent:text")).toHaveLength(3);
```

## Fixture Structure

```json
{
  "metadata": {
    "recordedAt": "2024-01-15T10:30:00Z",
    "version": "1.0.0"
  },
  "input": {
    "prompt": "Analyze this document"
  },
  "events": [
    {
      "event": { "type": "task:start", "taskId": "t1" },
      "context": { "sessionId": "s1", "runId": "r1" },
      "timestamp": 1705312200000
    },
    {
      "event": { "type": "agent:text", "content": "..." },
      "context": { "sessionId": "s1", "runId": "r1", "agentName": "analyzer" },
      "timestamp": 1705312201000
    }
  ]
}
```

## Testing Patterns

### Behavior Verification

Assert on event patterns, not exact content:

```typescript
// Bad: brittle assertion on exact text
expect(events[0].event.content).toBe("Here are 3 points...");

// Good: behavioral assertion
expect(events.filter(e => e.event.type === "agent:text")).toHaveLength(1);
expect(events.some(e => e.event.type === "task:complete")).toBe(true);
```

### Event Sequence Testing

Verify correct order:

```typescript
const types = events.map(e => e.event.type);
expect(types).toEqual([
  "task:start",
  "agent:text",
  "task:complete"
]);
```

### Error Path Testing

Record error scenarios:

```typescript
// Record with intentional error
const errorFixture = await recordScenario({
  input: { invalid: true }
});

// Replay to test error handling
const events = await replay(errorFixture);
expect(events.some(e => e.event.type === "task:failed")).toBe(true);
```

## Benefits

### Speed

Replay is instant - no network calls:

```typescript
// Live: 2000ms
// Replay: 2ms
```

### Cost

No API costs for test runs:

```typescript
// Live: $0.01/test
// Replay: $0.00/test
```

### Determinism

Same fixture = same behavior:

```typescript
// Run 1: passes
// Run 100: still passes
```

### Offline Testing

No internet required:

```bash
# Works on airplane
bun test
```

## Trade-offs

### Fixture Maintenance

Fixtures can become stale:
- Agent behavior changes → fixtures need updating
- New events added → fixtures need updating

Mitigate with fixture versioning and automated re-recording.

### Coverage Gaps

Fixtures only test recorded paths:
- Edge cases not recorded = not tested

Mitigate with comprehensive recording sessions.

### Initial Recording

First recording still needs live API:

```typescript
// Must run once with real API
bun run record-fixtures
```

## Best Practices

### Version Fixtures

Include version in fixtures:

```json
{
  "metadata": {
    "version": "1.0.0",
    "harness": "researcher",
    "recordedWith": "claude-sonnet-4-20250514"
  }
}
```

### Automate Re-Recording

Script fixture updates:

```bash
# Re-record all fixtures
bun run record --all

# Re-record specific scenario
bun run record --scenario happy-path
```

### Test Structure

Organize by scenario:

```
tests/
  fixtures/
    researcher/
      happy-path.json
      empty-results.json
      rate-limited.json
    analyzer/
      simple-doc.json
      complex-doc.json
```

## Next Steps

- [Conformance Testing](/docs/concepts/testing/conformance) - Testing node types
- [Test Infrastructure](/docs/guides/testing/test-setup) - Setting up tests
- [Recording Fixtures](/docs/guides/testing/recording-fixtures) - How to record
