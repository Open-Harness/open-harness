agent: ambiguity-checker
timestamp: "2025-12-26T14:00:00Z"
feature_dir: specs/005-monologue-system
summary: "163 items checked. 7 ambiguities found (1 critical, 4 medium, 2 low)."
statistics:
  items_checked: 163
  vague_terms: 4
  placeholders: 0
  unmeasurable: 3
  total_findings: 7
  severity_breakdown:
    critical: 1
    medium: 4
    low: 2

findings:
  - id: A001
    type: vague_term
    location: spec.md:L28
    text: "the system uses sensible defaults (haiku model, buffer size 2, history size 5)"
    issue: "Term 'sensible' is subjective and not qualified. While specific values are provided in parentheses, the justification for choosing those values is not documented."
    recommendation: "Either remove 'sensible' and state directly: 'defaults to: haiku model, buffer size 2, history size 5', OR add rationale for why these values are sensible (e.g., 'haiku model for <500ms performance, buffer size 2 for low-latency first narrative, history size 5 for 3-4 narrative context window')"
    severity: low

  - id: A002
    type: vague_term
    location: spec.md:L44
    text: "each call generates appropriate narratives without duplicate setup"
    issue: "'Appropriate narratives' is undefined - no criterion for what makes a narrative 'appropriate' vs 'inappropriate'"
    recommendation: "Define: 'each call generates narratives that reference only events from that specific call scope' or 'narratives reflect only the work performed in that invocation'"
    severity: medium

  - id: A003
    type: vague_term
    location: spec.md:L160
    text: "Haiku model is fast enough for inline narrative generation without noticeably impacting task execution latency"
    issue: "'Noticeably' and 'fast enough' are subjective. The assumption states 'fast' but plan.md specifies <500ms as the target. The threshold is hidden in plan.md (L18) rather than documented in the assumption."
    recommendation: "Revise assumption to state explicit threshold: 'Haiku model generates narratives in <500ms per invocation, ensuring inline generation does not add more than 5% overhead to task execution time'"
    severity: medium

  - id: A004
    type: unmeasurable
    location: spec.md:L68
    text: "Disconnected narratives are confusing. 'I'm reading files' followed by 'I'm reading files' is useless."
    issue: "Narrative quality ('confusing', 'useless') is subjective. No acceptance criterion for how to verify narratives avoid repetition or maintain continuity."
    recommendation: "Add success criteria: 'Later narratives must reference work from previous narratives (test via text similarity check or manual review of sequence)' or 'History context reduces topic repetition across consecutive narratives (measure: < 20% term overlap between consecutive narratives)'"
    severity: medium

  - id: A005
    type: vague_term
    location: spec.md:L80
    text: "balance detail vs. noise"
    issue: "'Detail' and 'noise' are qualitative terms without measurable boundaries"
    recommendation: "Replace with: 'balance between frequent updates (small buffer) and consolidated summaries (large buffer)' and provide examples: 'minBufferSize=1 generates narrative every ~2-3 events; minBufferSize=5 generates after batches of 5+ events'"
    severity: low

  - id: A006
    type: unmeasurable
    location: spec.md:L102
    text: "Configure a mock LLM that throws errors. Run TaskHarness and verify tasks complete successfully with error logged but no narrative events."
    issue: "Acceptance criteria incomplete: 'with error logged' does not specify log level, location, or content. 'no narrative events' is vague—does this mean no event emission or no text content?"
    recommendation: "Clarify: 'Verify (a) error is logged at warn level to stderr/logger with message containing failure reason, (b) NarrativeEntry is not emitted for that invocation, (c) subsequent narrative generations succeed if LLM recovers'"
    severity: critical

  - id: A007
    type: vague_term
    location: research.md:L90
    text: "Skip trivial events, wait for meaningful work"
    issue: "'Trivial' and 'meaningful' are undefined—no criteria for what constitutes trivial vs meaningful events"
    recommendation: "Define via system prompt: 'Skip events where the payload is empty or represents system artifacts (e.g., cache checks, repeated retries). Focus on events representing user-facing work (file reads, code changes, validation results)' or add specifics in prompts.ts with examples"
    severity: medium

findings_summary: |
  **Key Findings:**

  1. **Critical (1)**: Acceptance test for error handling (A006) uses vague language ('error logged', 'no narrative events') without specifying where, how, or what to measure.

  2. **Medium Ambiguities (4)**:
     - A002: "Appropriate narratives" lacks definition
     - A003: "Fast enough" assumption conflicts with hidden <500ms threshold in plan.md
     - A004: Narrative quality metrics are subjective (what counts as "confusing"?)
     - A007: "Trivial" vs "meaningful" events undefined for LLM decision logic

  3. **Low Ambiguities (2)**:
     - A001: "Sensible defaults" justified only by values, not by reasoning
     - A005: "Balance detail vs. noise" lacks measurable distinction

  **Impact Assessment:**
  - None block implementation (contractor has context from plan.md/research.md)
  - All are refinement opportunities to reduce interpretation variance
  - A006 should be clarified before implementation to enable proper error test validation

recommendations_for_improvement:
  - Move performance threshold from plan.md:L18 into spec.md assumptions for visibility
  - Add explicit criteria to spec.md for narrative quality (repetition check, history usage)
  - Clarify all test expectations with measurable outcomes (log levels, event field checks)
  - Update research.md:L90 to provide concrete examples of "trivial" event types
  - Consider adding a "Verification Checklist" section to each user story with specific assertions
