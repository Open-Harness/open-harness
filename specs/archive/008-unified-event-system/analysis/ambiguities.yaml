agent: ambiguity-checker
timestamp: "2025-12-27T00:00:00Z"
feature_dir: /Users/abuusama/projects/dao/dao-spec-kit/specs/008-unified-event-system
summary: "283 items checked. 11 ambiguities found (1 critical, 7 medium, 3 low)."
statistics:
  items_checked: 283
  vague_terms: 5
  placeholders: 1
  unmeasurable: 5
  total_findings: 11
  severity_breakdown:
    critical: 1
    medium: 7
    low: 3

findings:
  - id: A001
    type: vague_term
    location: spec.md:L73
    text: "Without a clean renderer API, the unified bus is just infrastructure with no DX improvement."
    issue: "'clean' is undefined and subjective. No metric for API quality or cleanliness."
    recommendation: "Define measurable API quality criteria: e.g., 'API accepts no more than 3 required parameters', 'Enables type inference without manual type hints', 'Reduces boilerplate by >50% vs manual bus.subscribe()'"
    severity: medium

  - id: A002
    type: vague_term
    location: spec.md:L141
    text: "Observer pattern — listeners should not crash the emitter"
    issue: "'should not crash' is a behavior assumption, not quantifiable. What constitutes a 'crash'? Silent failure? Exception propagation? Partial delivery?"
    recommendation: "State explicitly: 'When a listener throws an error, the emitter (1) logs the error via console.error, (2) continues delivery to remaining listeners, (3) does NOT re-throw or propagate the error'"
    severity: medium

  - id: A003
    type: vague_term
    location: spec.md:L250
    text: "The ~5-10% performance overhead of AsyncLocalStorage is acceptable for this use case."
    issue: "'acceptable' is subjective. No baseline specified. Acceptable to whom? What measurement methodology?"
    recommendation: "Replace with measurable acceptance criteria: 'emit() latency <1ms for typical use case (5 listeners, 3 patterns each). Memory overhead <2MB per 100 concurrent scopes. Measured via benchmark suite.'"
    severity: medium

  - id: A004
    type: vague_term
    location: spec.md:L260
    text: "This feature does NOT guarantee strict event ordering (best-effort delivery)."
    issue: "'best-effort' is undefined. What level of effort? What success rate is acceptable? How does implementation differ from strict?"
    recommendation: "Define: 'Events are delivered in the order emit() was called within a single scope. No ordering guarantee across parallel scopes. Implementation uses synchronous listener dispatch (no queuing).'"
    severity: medium

  - id: A005
    type: unmeasurable
    location: spec.md:L105
    text: "Consistent envelope enables generic event handling (logging, replay, debugging)."
    issue: "Success criterion uses 'enables' without defining what success looks like. How to verify envelope achieves generic handling? What qualifies as 'generic'?"
    recommendation: "Add measurable test: 'A generic logging handler can consume any EnrichedEvent without type guards. Test handler logs 50+ event types without modification.'"
    severity: medium

  - id: A006
    type: unmeasurable
    location: plan.md:L54
    text: "⚠️ MONITOR | Bus → Agents → Bus possible; use tokens to break cycle"
    issue: "Monitoring criterion lacks objective conditions. When should monitoring trigger escalation? What threshold indicates a problem?"
    recommendation: "Replace with explicit rule: 'All DI token bindings must pass circular dependency check via: npm list --duplicate. Circular imports prohibited.'"
    severity: low

  - id: A007
    type: unmeasurable
    location: plan.md:L142
    text: "Rationale: The existing EventBus and HarnessInstance code is the authoritative source. Exclude example harnesses to prevent implementation drift toward specific use cases rather than the general unified system."
    issue: "'prevent implementation drift' is subjective. How to verify drift didn't occur? What constitutes drift?"
    recommendation: "Quantify: 'Implementation must pass integration tests against 3+ different harness patterns (parallel, sequential, nested). No tests using examples/ harnesses.'"
    severity: low

  - id: A008
    type: placeholder
    location: quickstart.md:L147
    text: "const taskId = event.context.task?.id ?? \"???\";"
    issue: "Placeholder string '???' in example code. Suggests incomplete example or undefined fallback behavior."
    recommendation: "Define expected behavior: e.g., 'const taskId = event.context.task?.id ?? \"unknown\"; // Log warning if task context missing'"
    severity: medium

  - id: A009
    type: unmeasurable
    location: tasks.md:L275
    text: "Add JSDoc comments to all public APIs"
    issue: "'all public APIs' is vague. What constitutes 'complete' JSDoc? What fields are mandatory (description, @param, @returns, @example, @throws)?"
    recommendation: "Specify: 'JSDoc must include: (1) one-line description, (2) @param with types for all params, (3) @returns with type, (4) @example for complex APIs, (5) @throws for error cases'"
    severity: low

  - id: A010
    type: vague_term
    location: research.md:L186
    text: "Scalability limit: Becomes problematic at >100 listeners with >20 patterns each. Not a concern for this system (max ~10 renderers typically)."
    issue: "'typically' is vague. What assumptions support 'max ~10 renderers'? What happens if assumption violated? Should this be a hard limit?"
    recommendation: "Quantify: 'System supports up to 10 concurrent renderers per harness (measured in acceptance tests T063-T064). If >10 renderers attached, emit() latency may exceed 1ms threshold. Document constraint in JSDoc.'"
    severity: medium

  - id: A011
    type: unmeasurable
    location: tasks.md:L159
    text: "Task file paths match actual created/modified files"
    issue: "Verification criterion is binary but implementation context unclear. How strictly should paths match? Allow relative vs absolute? Allow refactored paths?"
    recommendation: "Define: 'File paths must match actual created/modified files exactly (absolute paths, case-sensitive). Run 'git status --short' against task list to verify all paths have corresponding file changes.'"
    severity: low

quality_notes:
  - "Overall document quality is high with explicit acceptance scenarios and measurable success criteria."
  - "Most issues are in descriptive/rationale sections rather than requirements."
  - "Functional requirements (FR-001 through FR-007) are well-specified with clear acceptance scenarios."
  - "Success criteria (SC-001 through SC-007) are mostly measurable with explicit test names."
  - "Main gaps: (1) API quality metrics in US4, (2) Performance baseline definition, (3) Failure handling behavior clarity"

recommendations_priority:
  - "CRITICAL (A008): Remove placeholder '???' from quickstart code - confuses implementations."
  - "HIGH (A003, A004, A010): Define performance baselines before implementation - will inform optimization decisions."
  - "MEDIUM (A001, A002, A005): Expand API quality and event handling specs - affects renderer implementation."
  - "LOW (A006, A007, A009, A011): Clarify monitoring/documentation criteria - more style than substance."
